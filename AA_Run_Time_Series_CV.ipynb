{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "AVzuK2SJtv1Oy2arpvkBFg",
     "report_properties": {},
     "type": "CODE"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lightgbm in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (3.3.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (from lightgbm) (1.21.6)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (1.7.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (from xgboost) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: colorlog in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (4.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: cliff in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (4.0.0)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (from optuna) (1.21.6)\n",
      "Requirement already satisfied: alembic in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.4.32)\n",
      "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.7.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: Mako in c:\\programdata\\anaconda3\\lib\\site-packages (from alembic->optuna) (1.2.2)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from cliff->optuna) (4.0.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from cliff->optuna) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.4)\n",
      "Requirement already satisfied: pyreadline3 in c:\\programdata\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (3.4.1)\n",
      "Requirement already satisfied: pyperclip>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->cliff->optuna) (3.7.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stevedore>=2.0.1->cliff->optuna) (5.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Mako->alembic->optuna) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.7)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\andrew_lazzeri\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Import base packages\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset, MonthEnd\n",
    "import scipy.linalg as la\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "\n",
    "#Set seeds for different algorithms\n",
    "import random\n",
    "random.seed(200)\n",
    "np.random.seed(200)\n",
    "\n",
    "#Setup directory and pickling\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#Help ignore the high verbosity in optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Import sklearn machine learning packages\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Import LGBM\n",
    "!pip install lightgbm\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "#Import XGBoost\n",
    "!pip install xgboost\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Import Optuna tuning hyperparameter\n",
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_slice\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "#Keras imports for NN\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow.python.keras import regularizers\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import utils\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras import datasets\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "from keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "!pip install numpy\n",
    "!pip install tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv2D, Flatten, Dropout, Activation, Reshape, TimeDistributed, Conv1D, RepeatVector\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "#Setup an initial keras random seed - will be superceded by other random seeds in ensembling\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "#The below objectives set the optimization criteria by model\n",
    "#'n_components':[i for i in range(1, 31, 1)]\n",
    "#!pip install optkeras\n",
    "#import optkeras\n",
    "#optkeras.optkeras.get_trial_default = lambda: optuna.trial.FrozenTrial(\n",
    "#            None, None, None, None, None, None, None, None, None, None, None)\n",
    "#from optkeras.optkeras import OptKeras\n",
    "#import tensorflow.keras.backend as K\n",
    "#from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n",
    "#from optkeras.optkeras import OptKeras\n",
    "#sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Aw3yGHswaHDIw0jS8TZiVa",
     "report_properties": {},
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#Upload Stock & Bond data\n",
    "sb_df = pd.read_csv(\"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/Stock_Bond_Rets.csv\")\n",
    "sb_df['Date'] = pd.to_datetime(sb_df['Unnamed: 0'])\n",
    "sb_df = sb_df.drop(['Unnamed: 0', 'MLPs', 'REITs', 'TIPS'], axis=1)\n",
    "sb_df = sb_df.set_index(['Date'])\n",
    "sb_synched_df = pd.DataFrame(sb_df.iloc[2::, :].astype(float)/100, columns = sb_df.columns)\n",
    "sb_synched_df = sb_synched_df.dropna()\n",
    "\n",
    "#Upload Commodity Data\n",
    "commodity_df = pd.read_csv(\"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/CommodityIndex.csv\")\n",
    "commodity_df['Date'] = pd.to_datetime(commodity_df['Date'].astype(str).str[0:8])\n",
    "commodity_df = commodity_df.set_index('Date')\n",
    "\n",
    "#Create a combined dataframe\n",
    "combined_df = pd.DataFrame(index = sb_synched_df.index)\n",
    "combined_df = combined_df.join(commodity_df)\n",
    "combined_df = combined_df.pct_change(periods = 1)\n",
    "combined_df = combined_df.join(sb_synched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5niW2cWjT1ZrFQWEDqL7so",
     "report_properties": {},
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "## Set Parameters ##\n",
    "cov_window = 63 #Lookback window for the covariance matrix\n",
    "days_fwd = 63 #Forward window for realized covariance matrix target\n",
    "annualization_adj = 252 #Adjustment to annualize the covariance matrix (i.e. 252 for trading days)\n",
    "initial_train_window = 2*252 #Required observations prior to first training\n",
    "retrain_intervals = 252 #Amount of time between training intervals\n",
    "CV_budget = 3 #Cross validations hyper parameter tuning for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "g6Dn836hS2O5l5VzFzngtl",
     "report_properties": {},
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#Generating a series of covariance matrix\n",
    "def create_tr_covar(combined_df, look_back, annualization_adj):\n",
    "    covar_mat = combined_df.rolling(window = look_back).cov()\n",
    "    covar_mat = covar_mat.dropna() * annualization_adj\n",
    "    covar_mat.index.rename(['Date', 'Commodity'], inplace=True)\n",
    "    return covar_mat\n",
    "\n",
    "def create_exp_tr_covar(combined_df, look_back, annualization_adj):\n",
    "    exp_covar_mat = combined_df.ewm(span=look_back).cov()\n",
    "    exp_covar_mat = exp_covar_mat.dropna() * annualization_adj\n",
    "    exp_covar_mat.index.rename(['Date', 'Commodity'], inplace=True)\n",
    "    return exp_covar_mat\n",
    "\n",
    "def Y_variable(covar_mat, fwd_period):\n",
    "    y_covar_mat = covar_mat.copy()\n",
    "    y_covar_mat = y_covar_mat.reset_index()\n",
    "    y_covar_mat['Date'] = y_covar_mat['Date'] - DateOffset(days = fwd_period)\n",
    "    y_covar_mat = y_covar_mat.set_index(['Date', 'Commodity'])\n",
    "    return y_covar_mat\n",
    "\n",
    "def data_transform(original_mat):\n",
    "    Lambda, Q = np.linalg.eig(original_mat)\n",
    "    S = np.dot(Q,np.dot(np.diag(np.sqrt(Lambda)),Q.T))\n",
    "    C = la.cholesky(S)\n",
    "    return C\n",
    "\n",
    "def to_rcov(transformed_mat):\n",
    "    S = np.dot(transformed_mat.T, transformed_mat)\n",
    "    Lambda, Q = np.linalg.eig(S)\n",
    "    rcov = np.dot(Q,np.dot(np.diag(np.square(Lambda)),Q.T))\n",
    "    return rcov\n",
    "\n",
    "def generate_matrix(covar_mat):\n",
    "    dates = sorted(list(set(covar_mat.index.get_level_values('Date'))))\n",
    "    transformed_df = pd.DataFrame()\n",
    "    for date in dates:\n",
    "        iter_df = covar_mat[covar_mat.index.get_level_values('Date')==date]\n",
    "        try:\n",
    "            cholesky_mat = data_transform(iter_df)\n",
    "            cholesky_df = pd.DataFrame(cholesky_mat, index = iter_df.index, columns = iter_df.columns)\n",
    "            transformed_df = pd.concat([transformed_df, cholesky_df])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return transformed_df \n",
    "\n",
    "def ML_feature_generator(cov_df:pd.DataFrame()) -> pd.DataFrame():\n",
    "    melt_C_df = pd.melt(cov_df.reset_index(), id_vars = ['Date', 'Commodity'])\n",
    "    melt_C_df = melt_C_df[melt_C_df['value']!=0]\n",
    "    melt_C_df['FinalIdx'] = melt_C_df['Commodity'].str.split(\"_\", expand = True)[0] + '_' + melt_C_df['variable'].str.split(\"_\", expand = True)[0] \n",
    "    melt_C_df = melt_C_df[['Date','value', 'FinalIdx']]\n",
    "    melt_C_df = melt_C_df.sort_values(['Date', 'FinalIdx'], ascending = True)\n",
    "    melt_C_df = melt_C_df.set_index(['Date', 'FinalIdx'])\n",
    "    return melt_C_df\n",
    "\n",
    "def add_lags(cov_df, period_len, lags):\n",
    "    iter_df = cov_df.copy()\n",
    "    dates = sorted(list(set(iter_df.index.get_level_values('Date'))))\n",
    "    lag_df = pd.DataFrame(index = dates)\n",
    "    iter_pivot = iter_df.reset_index().pivot(index = 'Date', columns = 'FinalIdx', values = 'value')\n",
    "    \n",
    "    for i in range(0, lags*period_len+1, period_len):\n",
    "        lag_add = iter_pivot.shift(i).copy()\n",
    "        lag_add = lag_add.add_suffix('Lag_'+str(i))\n",
    "        lag_df = lag_df.join(lag_add)\n",
    "        \n",
    "    return lag_df\n",
    "\n",
    "def backfill_covar(covar_df, resample_freq):\n",
    "    covar_copy = covar_df.copy()\n",
    "    min_date = min(covar_df.index.get_level_values('Date'))\n",
    "    max_date = max(covar_df.index.get_level_values('Date'))\n",
    "    dates = pd.date_range(min_date, max_date, freq = resample_freq)\n",
    "    resampled_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(dates)):\n",
    "        if (covar_df[covar_df.index.get_level_values('Date') == dates[i]].shape[0] != 0):\n",
    "            iter_covar_df = covar_copy[covar_copy.index.get_level_values('Date') == dates[i]]\n",
    "            resampled_df = pd.concat([resampled_df, iter_covar_df])\n",
    "        elif (covar_df[covar_df.index.get_level_values('Date') == dates[i]].shape[0] == 0):\n",
    "            last_time_stamp = max(resampled_df.index.get_level_values('Date'))\n",
    "            iter_covar_df = resampled_df[resampled_df.index.get_level_values('Date') == last_time_stamp]\n",
    "            iter_covar_df = iter_covar_df.reset_index()\n",
    "            iter_covar_df['Date'] = pd.to_datetime(dates[i])\n",
    "            iter_covar_df = iter_covar_df.set_index(['Date', 'Commodity'])\n",
    "            resampled_df = pd.concat([resampled_df, iter_covar_df])\n",
    "   \n",
    "    return resampled_df\n",
    "\n",
    "X_df = generate_matrix(create_tr_covar(combined_df, cov_window, annualization_adj))\n",
    "y_df = generate_matrix(Y_variable(create_tr_covar(combined_df, cov_window, annualization_adj), days_fwd))\n",
    "y_df = backfill_covar(y_df, 'D')\n",
    "C_df_X = ML_feature_generator(X_df)\n",
    "C_df_y = ML_feature_generator(y_df)\n",
    "C_df_X_lag = add_lags(C_df_X, 5, 52)\n",
    "df_y = C_df_y.reset_index().pivot(index = 'Date', columns = 'FinalIdx', values = 'value')\n",
    "df_y = df_y.add_prefix('Y_')\n",
    "combined_df = C_df_X_lag.join(df_y)\n",
    "combined_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_df_splits(combined_df, date):\n",
    "    #Split the data by time\n",
    "    train_df = combined_df[combined_df.index < date]\n",
    "    predict_df = combined_df[combined_df.index == date]\n",
    "    \n",
    "    #Split the data by features and targets\n",
    "    train_x = train_df[[col for col in train_df.columns if 'Y_' not in col]]\n",
    "    train_y = train_df[[col for col in train_df.columns if 'Y_' in col]]\n",
    "    predict_x = predict_df[[col for col in predict_df.columns if 'Y_' not in col]]\n",
    "    predict_y = predict_df[[col for col in predict_df.columns if 'Y_' in col]]\n",
    "    \n",
    "    return train_x, train_y, predict_x, predict_y\n",
    "\n",
    "#This simply creates an object to avoid an error downstream and is irrelevant otherwise\n",
    "train_x, train_y, predict_x, predict_y = generate_df_splits(combined_df, '2003-12-31')\n",
    "\n",
    "def apply_robust_scaler(train_x, predict_x):\n",
    "    transformer = RobustScaler(quantile_range=(1.0, 99.0)).fit(train_x)\n",
    "    train_x = pd.DataFrame(transformer.transform(train_x), index = train_x.index, columns = train_x.columns)\n",
    "    predict_x = pd.DataFrame(transformer.transform(predict_x), index = predict_x.index, columns = predict_x.columns)\n",
    "    return train_x, predict_x\n",
    "\n",
    "def ts_cross_validation(train_x, train_y, model, ts_folds = 3):\n",
    "    combined_df = train_x.join(train_y)\n",
    "    col_len = len(combined_df.index)\n",
    "    rough_splits = col_len // (ts_folds + 1)\n",
    "    errors = []\n",
    "    idx = [i for i in range(0, combined_df.shape[0], rough_splits)]\n",
    "    \n",
    "    for i in range(2, len(idx)):\n",
    "        iter_train = combined_df.iloc[idx[i-2]:idx[i-1], :] \n",
    "        iter_test = combined_df.iloc[idx[i-1]:idx[i], :]\n",
    "        iter_train_x = iter_train[train_x.columns]\n",
    "        iter_test_x = iter_test[train_x.columns]\n",
    "        iter_train_x, iter_test_x = apply_robust_scaler(iter_train_x, iter_test_x)\n",
    "        \n",
    "        model.fit(iter_train_x, iter_train[train_y.columns])\n",
    "        MSE = mean_squared_error(iter_test[train_y.columns], np.nan_to_num(model.predict(iter_test_x)))\n",
    "        errors.append(MSE)\n",
    "    return np.mean(errors)\n",
    "\n",
    "def objective_XGB(trial, train_x = train_x, train_y = train_y):\n",
    "    \n",
    "    XGB_params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [x for x in range(50, 1000, 50)]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', list(np.logspace(-2, .1, 20))),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [x for x in range(1, 50)]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [x/10 for x in range(1, 10)]),\n",
    "        'reg_lambda': trial.suggest_categorical('reg_lambda', list(np.logspace(0, 2, 20))),\n",
    "        'reg_alpha': trial.suggest_categorical('reg_alpha', list(np.logspace(0, 2, 20))),\n",
    "        'gamma': trial.suggest_categorical('gamma', list(np.logspace(-2, -.25, 20))),\n",
    "        'verbosity': trial.suggest_categorical('verbosity', [0])\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**XGB_params)  \n",
    "    MSE = ts_cross_validation(train_x, train_y, model)\n",
    "    return MSE\n",
    "\n",
    "def objective_NN3(trial, train_x = train_x, train_y = train_y):\n",
    "    K.clear_session() \n",
    "            \n",
    "    #Setup the model hyperparameter search\n",
    "    activation_fx = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\"])\n",
    "    layer_units = trial.suggest_categorical('units', [64*4, 64*3, 64*2, 64*1])\n",
    "    lr = trial.suggest_categorical('learning_rate', [.001, .01, .1])\n",
    "    reg = trial.suggest_categorical('reg', [10**-3, 10**-5])\n",
    "    droprate = trial.suggest_categorical('dropout', [.20, .30, .4])\n",
    "    epochs = trial.suggest_categorical('epochs', [100])\n",
    "    batch_n = trial.suggest_categorical('batch_size', [64, 64*2])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "        \n",
    "    #Setup the model structure\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = layer_units, activation = activation_fx,  kernel_initializer='he_uniform',\n",
    "                    kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = layer_units/2, activation = activation_fx,  kernel_initializer='he_uniform', \n",
    "                    kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units = layer_units/4, activation = activation_fx,  kernel_initializer='he_uniform',\n",
    "                    kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_y.shape[1], kernel_initializer='normal',activation='linear'))\n",
    "    model.compile(optimizer = Adam(learning_rate = lr), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    model.fit(train_x, train_y, validation_split = 0.2, shuffle = True, batch_size = batch_n, \n",
    "              epochs = epochs, verbose = 0, callbacks=[es])  \n",
    "    \n",
    "    MSE = ts_cross_validation(train_x, train_y, model)\n",
    "    return MSE\n",
    "\n",
    "\n",
    "def optimization_instance(objective_fx, trial_runs: int):\n",
    "    sampler = TPESampler(seed=200)  # Make the sampler behave in a deterministic way.\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize') \n",
    "    study.optimize(objective_fx, n_trials=trial_runs)\n",
    "    if len(study.trials) % 50 == 0:\n",
    "        print(str(objective_fx), \" has completed \", len(study.trials)/trial_runs, \"of tuning trials.\")\n",
    " \n",
    "    return study\n",
    "\n",
    "def evaluation(study, model, train_x, train_y, predict_x, predict_y):\n",
    "    predict_model = model(**study.best_params)\n",
    "    train_x, predict_x = apply_robust_scaler(train_x, predict_x)\n",
    "    predict_model.fit(train_x, train_y)\n",
    "    #Generate a test score and prediction\n",
    "    y_pred = predict_model.predict(predict_x)\n",
    "    MSE = mean_squared_error(predict_y, np.nan_to_num(y_pred))\n",
    "    return y_pred, MSE, predict_y, predict_model\n",
    "\n",
    "def save_predictions(prediction_df, model_name, date, working_path):\n",
    "    #Save the prediction_df\n",
    "    prediction_df.to_csv(working_path + \"Prediction_dfs/\" + model_name + '_' + date + '.csv')\n",
    "    \n",
    "def save_ML_model(model, model_name, date, working_path):\n",
    "    filename = working_path + \"Models/\" + model_name + '_' + date + '.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))      \n",
    "    \n",
    "def save_NN_model(model, model_name, date, working_path):\n",
    "    filename = working_path + \"Models/\" + model_name + '_' + date + '.h'\n",
    "    model.save(filename)\n",
    "\n",
    "def backtest_XGB(combined_df, min_train_window, re_train_periods, runs):\n",
    "    dates = sorted(set(list(combined_df.index)))\n",
    "    model = XGBRegressor\n",
    "    prediction_df = pd.DataFrame(columns = [col for col in combined_df.columns if 'Y_' in col])\n",
    "       \n",
    "    for i in range(min_train_window,len(dates)):\n",
    "        train_x, train_y, predict_x, predict_y = generate_df_splits(combined_df, dates[i])\n",
    "               \n",
    "        if i % re_train_periods == 0:\n",
    "            #Train the XGBoost Model\n",
    "            start_time = time.time()\n",
    "            study = optimization_instance(objective_XGB, runs)\n",
    "            train_x, predict_x = apply_robust_scaler(train_x, predict_x)\n",
    "            y_pred, MSE, predict_y, predict_model = evaluation(study, model, train_x, train_y, predict_x, predict_y)\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = train_y.columns, index = predict_x.index)])\n",
    "            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\n",
    "            save_predictions(prediction_df, 'XGB', str(dates[i])[0:10], working_path)\n",
    "            save_ML_model(predict_model, 'XGB', str(dates[i])[0:10], working_path)\n",
    "            end_time = time.time()\n",
    "            print(\"One training instance takes: \", (end_time - start_time)/60.0)\n",
    "            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\n",
    "                      \n",
    "        else:\n",
    "            train_x, predict_x = apply_robust_scaler(train_x, predict_x)\n",
    "            y_pred = predict_model.predict(predict_x)\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = train_y.columns, index = predict_x.index)])\n",
    "    \n",
    "    save_predictions(prediction_df, 'XGB', '_Final', working_path)\n",
    "    print(\"Run is now fully complete.\")\n",
    "    return prediction_df\n",
    "\n",
    "def NN_generator(best_params, layers, train_x, train_y):\n",
    "    \n",
    "    #Extract the best parameters\n",
    "    act = best_params['activation']\n",
    "    units = best_params['units']\n",
    "    lr = best_params['learning_rate']\n",
    "    reg = best_params['reg']\n",
    "    droprate = best_params['dropout']\n",
    "    epochs = best_params['epochs']\n",
    "    batch_n = best_params['batch_size']\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "      \n",
    "    #Setup the combined df\n",
    "    input_dim = train_x.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units,  kernel_initializer='he_uniform', input_dim = input_dim, activation = act, \n",
    "                    kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(units/(i*2), activation=act,  kernel_initializer='he_uniform',\n",
    "                        kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "        model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_y.shape[1], kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate = lr), loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    history = model.fit(train_x, train_y, validation_split = 0.2, shuffle = True, \n",
    "              batch_size = batch_n, epochs = epochs, verbose = 0, callbacks=[es])\n",
    "   \n",
    "    return model, history\n",
    "\n",
    "def NN_evaluator(best_params, layers, train_x, train_y, test_x, test_y):\n",
    "    model, history = NN_generator(best_params, layers, train_x, train_y)\n",
    "    iter_pred = model.predict(test_x, verbose = 0)\n",
    "    return iter_pred, model, history\n",
    "\n",
    "def backtest_NN(combined_df, min_train_window, re_train_periods, runs):\n",
    "    dates = sorted(set(list(combined_df.index)))\n",
    "    prediction_df = pd.DataFrame(columns = [col for col in combined_df.columns if 'Y_' in col])\n",
    "       \n",
    "    for i in range(min_train_window,len(dates)):\n",
    "        train_x, train_y, predict_x, predict_y = generate_df_splits(combined_df, dates[i])\n",
    "                \n",
    "        if i % re_train_periods == 0:\n",
    "            #Train the NN Model\n",
    "            start_time = time.time()\n",
    "            study = optimization_instance(objective_NN3, runs)\n",
    "            train_x, predict_x = apply_robust_scaler(train_x, predict_x)\n",
    "            y_pred, predict_model, history = NN_evaluator(study.best_params, 3, \n",
    "                                                          train_x, train_y, predict_x, predict_y)\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = train_y.columns, index = predict_x.index)])\n",
    "            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\n",
    "            save_predictions(prediction_df, 'NN3', str(dates[i])[0:10], working_path)\n",
    "            save_NN_model(predict_model, 'NN3', str(dates[i])[0:10], working_path)\n",
    "            end_time = time.time()\n",
    "            print(\"One training instance takes: \", (end_time - start_time)/60.0)\n",
    "            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\n",
    "                      \n",
    "        else:\n",
    "            train_x, predict_x = apply_robust_scaler(train_x, predict_x)\n",
    "            y_pred = predict_model.predict(predict_x, verbose = 0)\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = train_y.columns, index = predict_x.index)])\n",
    "    \n",
    "    save_predictions(prediction_df, 'NN3', '_Final', working_path)\n",
    "    print(\"Run is now fully complete.\")\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training instance takes:  137.49629204273225\n",
      "0.09  of the run is complete.\n",
      "One training instance takes:  141.75285143852233\n",
      "0.13  of the run is complete.\n",
      "One training instance takes:  147.19586871465046\n",
      "0.17  of the run is complete.\n",
      "One training instance takes:  151.48110322952272\n",
      "0.21  of the run is complete.\n",
      "One training instance takes:  155.71312948465348\n",
      "0.26  of the run is complete.\n",
      "One training instance takes:  160.53048454920452\n",
      "0.3  of the run is complete.\n",
      "One training instance takes:  164.47204701105753\n",
      "0.34  of the run is complete.\n",
      "One training instance takes:  170.63983585834504\n",
      "0.39  of the run is complete.\n",
      "One training instance takes:  171.92296189864476\n",
      "0.43  of the run is complete.\n",
      "One training instance takes:  177.0112081448237\n",
      "0.47  of the run is complete.\n",
      "One training instance takes:  180.44828838507334\n",
      "0.52  of the run is complete.\n",
      "One training instance takes:  183.93473306894302\n",
      "0.56  of the run is complete.\n",
      "One training instance takes:  187.81416420936586\n",
      "0.6  of the run is complete.\n",
      "One training instance takes:  192.16736420790355\n",
      "0.64  of the run is complete.\n",
      "One training instance takes:  197.00546682278315\n",
      "0.69  of the run is complete.\n",
      "One training instance takes:  200.44041019280752\n",
      "0.73  of the run is complete.\n",
      "One training instance takes:  203.79901212453842\n",
      "0.77  of the run is complete.\n",
      "One training instance takes:  207.7379113515218\n",
      "0.82  of the run is complete.\n",
      "One training instance takes:  211.74930308262506\n",
      "0.86  of the run is complete.\n",
      "One training instance takes:  216.47967130740483\n",
      "0.9  of the run is complete.\n",
      "One training instance takes:  220.9006727854411\n",
      "0.95  of the run is complete.\n",
      "One training instance takes:  225.78488186597824\n",
      "0.99  of the run is complete.\n",
      "Run is now fully complete.\n"
     ]
    }
   ],
   "source": [
    "#Run the XGBoost predictions\n",
    "XG_prediction_df = backtest_XGB(combined_df, initial_train_window, retrain_intervals, CV_budget)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7616 - mean_squared_error: 5.4587e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.3963 - mean_squared_error: 6.0746e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4092 - mean_squared_error: 5.4650e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2337 - mean_squared_error: 5.5808e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2455 - mean_squared_error: 7.8338e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2310 - mean_squared_error: 5.8587e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.4434e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 8.1922e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0023 - mean_squared_error: 5.5733e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.6357768177986145\n",
      "0.09  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7612 - mean_squared_error: 5.3338e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.3920 - mean_squared_error: 5.9988e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4071 - mean_squared_error: 5.4091e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2331 - mean_squared_error: 5.5605e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2453 - mean_squared_error: 7.8854e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2306 - mean_squared_error: 5.7938e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0021 - mean_squared_error: 5.2508e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0025 - mean_squared_error: 8.0763e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0025 - mean_squared_error: 5.8333e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.621719717979431\n",
      "0.13  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7598 - mean_squared_error: 5.4049e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4039 - mean_squared_error: 6.0783e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4125 - mean_squared_error: 5.4170e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2327 - mean_squared_error: 5.5747e-04\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2447 - mean_squared_error: 7.8391e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2303 - mean_squared_error: 5.8353e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0022 - mean_squared_error: 5.3354e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0026 - mean_squared_error: 8.3405e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.5494e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.6517183621724447\n",
      "0.17  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7581 - mean_squared_error: 5.3944e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4013 - mean_squared_error: 6.0387e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4130 - mean_squared_error: 5.4381e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2346 - mean_squared_error: 5.5597e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2462 - mean_squared_error: 7.9027e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2318 - mean_squared_error: 5.7781e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0023 - mean_squared_error: 5.3363e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0029 - mean_squared_error: 8.1970e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0027 - mean_squared_error: 5.6293e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.7085009376207987\n",
      "0.21  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7586 - mean_squared_error: 5.3786e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4021 - mean_squared_error: 6.0132e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4129 - mean_squared_error: 5.4452e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2337 - mean_squared_error: 5.5675e-04\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2456 - mean_squared_error: 7.8671e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2310 - mean_squared_error: 5.8016e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0029 - mean_squared_error: 5.5468e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0040 - mean_squared_error: 8.2332e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0037 - mean_squared_error: 5.5405e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.7117697397867837\n",
      "0.26  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7566 - mean_squared_error: 5.3850e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4109 - mean_squared_error: 5.9950e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - mean_squared_error: 5.3839e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2332 - mean_squared_error: 5.5923e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2450 - mean_squared_error: 7.8156e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2307 - mean_squared_error: 5.8493e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0021 - mean_squared_error: 5.0943e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 8.1446e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0024 - mean_squared_error: 5.7301e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.706006371974945\n",
      "0.3  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7595 - mean_squared_error: 5.4331e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.3993 - mean_squared_error: 6.0979e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4111 - mean_squared_error: 5.4756e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2332 - mean_squared_error: 5.5560e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2453 - mean_squared_error: 7.9004e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2307 - mean_squared_error: 5.7736e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0021 - mean_squared_error: 5.2041e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0024 - mean_squared_error: 8.2940e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0022 - mean_squared_error: 5.5810e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.7718979001045225\n",
      "0.34  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7604 - mean_squared_error: 5.4058e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4009 - mean_squared_error: 6.0652e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4124 - mean_squared_error: 5.4785e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2334 - mean_squared_error: 5.5543e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2454 - mean_squared_error: 7.9308e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2307 - mean_squared_error: 5.7448e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0021 - mean_squared_error: 5.1006e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 8.2563e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.7776e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.7781540234883626\n",
      "0.39  of the run is complete.\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.7595 - mean_squared_error: 5.4519e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4015 - mean_squared_error: 6.0988e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4122 - mean_squared_error: 5.5402e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2342 - mean_squared_error: 5.5426e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2458 - mean_squared_error: 7.9324e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2314 - mean_squared_error: 5.7184e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.3877e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0026 - mean_squared_error: 8.0667e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0023 - mean_squared_error: 5.7938e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.8028810461362204\n",
      "0.43  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7559 - mean_squared_error: 5.4435e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - mean_squared_error: 6.0907e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4237 - mean_squared_error: 5.5037e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2337 - mean_squared_error: 5.5716e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2454 - mean_squared_error: 7.8649e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2310 - mean_squared_error: 5.7929e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0028 - mean_squared_error: 5.5485e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0035 - mean_squared_error: 8.1486e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0030 - mean_squared_error: 5.5229e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.7885018507639567\n",
      "0.47  of the run is complete.\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.7578 - mean_squared_error: 5.4332e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.3992 - mean_squared_error: 6.1446e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4102 - mean_squared_error: 5.4490e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2328 - mean_squared_error: 5.5609e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2450 - mean_squared_error: 7.8914e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2305 - mean_squared_error: 5.7844e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0022 - mean_squared_error: 5.3490e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 8.1176e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0024 - mean_squared_error: 5.7378e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.8564114570617676\n",
      "0.52  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7585 - mean_squared_error: 5.5190e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4029 - mean_squared_error: 6.0805e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4146 - mean_squared_error: 5.4817e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2332 - mean_squared_error: 5.5684e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2452 - mean_squared_error: 7.8787e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2307 - mean_squared_error: 5.8056e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0022 - mean_squared_error: 5.3613e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0025 - mean_squared_error: 8.1690e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.7782e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.874340347448985\n",
      "0.56  of the run is complete.\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.7597 - mean_squared_error: 5.3983e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4025 - mean_squared_error: 6.0418e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4133 - mean_squared_error: 5.4467e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2327 - mean_squared_error: 5.5547e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2447 - mean_squared_error: 7.9229e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2304 - mean_squared_error: 5.7638e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0027 - mean_squared_error: 5.2971e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0031 - mean_squared_error: 8.0853e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 5.8707e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.832140012582143\n",
      "0.6  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7565 - mean_squared_error: 5.3941e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4096 - mean_squared_error: 6.0032e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4173 - mean_squared_error: 5.4211e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2330 - mean_squared_error: 5.6002e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2449 - mean_squared_error: 7.7800e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2304 - mean_squared_error: 5.8829e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.4115e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0029 - mean_squared_error: 8.3683e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 5.5914e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.8716593027114867\n",
      "0.64  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7582 - mean_squared_error: 5.4342e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4034 - mean_squared_error: 6.0233e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4127 - mean_squared_error: 5.4348e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2337 - mean_squared_error: 5.5470e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2456 - mean_squared_error: 7.9256e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2310 - mean_squared_error: 5.7644e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0026 - mean_squared_error: 5.4981e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0029 - mean_squared_error: 8.1855e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0025 - mean_squared_error: 5.7471e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.8923118392626446\n",
      "0.69  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7560 - mean_squared_error: 5.4333e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4126 - mean_squared_error: 6.0265e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - mean_squared_error: 5.3916e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2324 - mean_squared_error: 5.5467e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2446 - mean_squared_error: 7.9311e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2300 - mean_squared_error: 5.8022e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0023 - mean_squared_error: 5.3452e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0026 - mean_squared_error: 8.0710e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0022 - mean_squared_error: 5.5580e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.955367604891459\n",
      "0.73  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7588 - mean_squared_error: 5.3741e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.3956 - mean_squared_error: 6.0408e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4079 - mean_squared_error: 5.4295e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2337 - mean_squared_error: 5.5504e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2455 - mean_squared_error: 7.9222e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2311 - mean_squared_error: 5.7583e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0024 - mean_squared_error: 5.1522e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0031 - mean_squared_error: 8.3055e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0026 - mean_squared_error: 5.6801e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "One training instance takes:  2.954473594824473\n",
      "0.77  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7592 - mean_squared_error: 5.4192e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4155 - mean_squared_error: 6.0135e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4230 - mean_squared_error: 5.4491e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2341 - mean_squared_error: 5.5802e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2457 - mean_squared_error: 7.8457e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2314 - mean_squared_error: 5.8222e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0021 - mean_squared_error: 5.1466e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 8.2091e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0026 - mean_squared_error: 5.6687e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.9829835017522175\n",
      "0.82  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7564 - mean_squared_error: 5.4895e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4141 - mean_squared_error: 6.2110e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - mean_squared_error: 5.4964e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2336 - mean_squared_error: 5.5762e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2455 - mean_squared_error: 7.8521e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2309 - mean_squared_error: 5.8059e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 5.2740e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0032 - mean_squared_error: 8.2737e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 5.7972e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  2.99013295173645\n",
      "0.86  of the run is complete.\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.7572 - mean_squared_error: 5.4434e-04\n",
      "9/9 [==============================] - 0s 8ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4140 - mean_squared_error: 6.1040e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4199 - mean_squared_error: 5.5485e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2345 - mean_squared_error: 5.5654e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2459 - mean_squared_error: 7.8880e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2315 - mean_squared_error: 5.8211e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 5.5406e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0030 - mean_squared_error: 8.2700e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 5.7612e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  3.0474380016326905\n",
      "0.9  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7576 - mean_squared_error: 5.4404e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.3973 - mean_squared_error: 6.0587e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4092 - mean_squared_error: 5.4088e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2337 - mean_squared_error: 5.5816e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.2455 - mean_squared_error: 7.8198e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2312 - mean_squared_error: 5.8398e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0022 - mean_squared_error: 5.2208e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0025 - mean_squared_error: 8.3071e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0023 - mean_squared_error: 5.7664e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  3.0175756096839903\n",
      "0.95  of the run is complete.\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.7568 - mean_squared_error: 5.3933e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4087 - mean_squared_error: 6.0893e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - mean_squared_error: 5.4405e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2333 - mean_squared_error: 5.5440e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2451 - mean_squared_error: 7.9289e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.2306 - mean_squared_error: 5.7460e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0027 - mean_squared_error: 5.7813e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0035 - mean_squared_error: 8.2330e-04\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0030 - mean_squared_error: 5.6514e-04\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "One training instance takes:  3.1094722112019855\n",
      "0.99  of the run is complete.\n",
      "Run is now fully complete.\n"
     ]
    }
   ],
   "source": [
    "#Run the NN predictions\n",
    "NN3_prediction_df = backtest_NN(combined_df, initial_train_window, retrain_intervals, CV_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_df.add_prefix('Y_')\n",
    "RNN_synched = X_df.join(y_df)\n",
    "RNN_synched = RNN_synched.dropna()\n",
    "X_df = RNN_synched[[col for col in RNN_synched.columns if 'Y_' not in col]]\n",
    "y_df = RNN_synched[[col for col in RNN_synched.columns if 'Y_' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef backtest_RNN(X_df, y_df, min_train_window, re_train_periods, cov_window, runs):\\n    #Convert all observations into trailing dates\\n    y_train, X_train, dates = RNN_feeder_array(X_df, y_df, cov_window)\\n    prediction_df = pd.DataFrame(columns = [col for col in y_df.columns])\\n    \\n    for i in range(min_train_window,len(dates)):\\n        RNN_train_x = X_train[0:(i-1)]\\n        RNN_train_y = y_train[0:(i-1)]\\n        RNN_predict_x = X_train[i]\\n        RNN_predict_y = y_train[i]\\n        iter_index = y_df[y_df.index.get_level_values(\\'Date\\') == dates[i]].index\\n        scaled_train, scaled_predict = array_robust_scaler(RNN_train_x, RNN_predict_x.reshape(1, RNN_predict_x.shape[0], RNN_predict_x.shape[1]))\\n\\n        if i % re_train_periods == 0:\\n            #Train the RNN Model\\n            start_time = time.time()\\n            study = optimization_instance(objective_RNN, runs)\\n            y_pred, predict_model, history = RNN_evaluator(study.best_params, scaled_train, RNN_train_y, scaled_predict)\\n            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\\n            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\\n            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\\n            save_predictions(prediction_df, \\'RNN\\', str(dates[i])[0:10], working_path)\\n            save_NN_model(predict_model, \\'RNN\\', str(dates[i])[0:10], working_path)\\n            end_time = time.time()\\n            print(\"One training instance takes: \", (end_time - start_time)/60.0)\\n            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\\n        \\n        else:\\n            y_pred = predict_model.predict(scaled_predict, verbose = 0)\\n            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\\n            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\\n    \\n    save_predictions(prediction_df, \\'RNN\\', \\'_Final\\', working_path)\\n    print(\"Run is now fully complete.\")\\n    return prediction_df\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RNN_feeder_array(X_df, y_df, window_size):\n",
    "    dates = sorted(list(set(X_df.index.get_level_values('Date'))))\n",
    "    date_refs = []\n",
    "    RNN_x = []\n",
    "    RNN_y = []\n",
    "    for date in dates[window_size::]:\n",
    "        iter_x = X_df[X_df.index.get_level_values('Date') <= date]\n",
    "        RNN_y.append(y_df[y_df.index.get_level_values('Date') == date].values)\n",
    "        RNN_x.append(iter_x.iloc[(-1*window_size*X_df.shape[1])::,:].values)\n",
    "    return np.array(RNN_y), np.array(RNN_x), dates[window_size::]\n",
    "\n",
    "train_y, train_x, dates = RNN_feeder_array(X_df, y_df, cov_window)\n",
    "\n",
    "\n",
    "def array_robust_scaler(train_x, predict_x):\n",
    "    #Reshape the training array for robust scaler by stacking the array similar to a dataframe\n",
    "    scaler_train_x = train_x.flatten()\n",
    "    scaler_train_x = scaler_train_x.reshape(train_x.shape[0]*train_x.shape[1], train_x.shape[2])\n",
    "    transformer = RobustScaler(quantile_range=(1.0, 99.0)).fit(scaler_train_x)\n",
    "    scaler_train_x = transformer.transform(scaler_train_x)\n",
    "    scaler_train_x = scaler_train_x.reshape(train_x.shape[0], train_x.shape[1], train_x.shape[2])\n",
    "    #Reshape the prediction array like the above\n",
    "    scaler_predict_x = predict_x.flatten()\n",
    "    scaler_predict_x = scaler_predict_x.reshape(predict_x.shape[0]*predict_x.shape[1], predict_x.shape[2])\n",
    "    scaler_predict_x = transformer.transform(scaler_predict_x)\n",
    "    scaler_predict_x = scaler_predict_x.reshape(predict_x.shape[0], predict_x.shape[1], predict_x.shape[2])\n",
    "    return scaler_train_x, scaler_predict_x\n",
    "\n",
    "def array_MSE(y_array, pred_array):\n",
    "    iter_y = np.nan_to_num(y_array, copy=True, nan=0, posinf=0, neginf=None)\n",
    "    iter_pred = np.nan_to_num(pred_array, copy=True, nan=0, posinf=0, neginf=None)\n",
    "    MSE = np.mean(np.power((iter_y - iter_pred), 2))\n",
    "    return MSE\n",
    "\n",
    "def ts_RNN_cross_validation(X_array, y_array, model, ts_folds = 4):\n",
    "    array_len = X_array.shape[0]\n",
    "    rough_splits = array_len // (ts_folds + 1)\n",
    "    \n",
    "    errors = []\n",
    "    idx = [i for i in range(0, array_len, rough_splits)]\n",
    "\n",
    "    for i in range(2, len(idx)):\n",
    "        iter_train_x = X_array[idx[i-2]:idx[i-1]]\n",
    "        iter_test_x = X_array[idx[i-1]:idx[i]]\n",
    "        iter_train_y = y_array[idx[i-2]:idx[i-1]]\n",
    "        iter_test_y = y_array[idx[i-1]:idx[i]]\n",
    "        iter_train_x, iter_test_x = array_robust_scaler(iter_train_x, iter_test_x)\n",
    "        \n",
    "        model.fit(iter_train_x, iter_train_y, verbose=0)\n",
    "        MSE = array_MSE(iter_test_y, model.predict(iter_test_x, verbose=0))\n",
    "        errors.append(MSE)\n",
    "        \n",
    "    return np.mean(errors)\n",
    "\n",
    "\n",
    "def objective_CNN(trial, train_x = train_x, train_y = train_y):\n",
    "    K.clear_session() \n",
    "            \n",
    "    #Setup the model hyperparameter search\n",
    "    activation_fx = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"LeakyReLU\"])\n",
    "    conv_filters = trial.suggest_categorical(\"filters\", [3, 5, 7, 9, 11, 13, 15])\n",
    "    LSTM_layers = trial.suggest_categorical(\"LSTM_layers\", [i for i in range(10, 100, 10)])\n",
    "    dense_layers = trial.suggest_categorical(\"Dense_layers\", [train_x.shape[2]*i for i in range(10)])\n",
    "    lr = trial.suggest_categorical('learning_rate', [.001, .01, .1])\n",
    "    reg = trial.suggest_categorical('reg', [10**-3, 10**-5])\n",
    "    droprate = trial.suggest_categorical('dropout', [.20, .30, .4])\n",
    "    epochs = trial.suggest_categorical('epochs', [100])\n",
    "    batch_n = trial.suggest_categorical('batch_size', [64, 64*2])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "    \n",
    "    #Setup the model structure\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(conv_filters, conv_filters, activation = activation_fx,\n",
    "                     padding = 'valid', input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dense_layers, activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(dense_layers/2), activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_x.shape[2]*train_x.shape[2], activation='linear'))\n",
    "    model.add(Reshape((train_x.shape[2], train_x.shape[2])))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=\"mean_squared_error\", metrics='mean_squared_error')\n",
    "    model.fit(train_x, train_y, validation_split = 0.2, shuffle = True, batch_size = batch_n, \n",
    "              epochs = epochs, verbose = 0, callbacks=[es]) \n",
    "    MSE = ts_RNN_cross_validation(train_x, train_y, model)\n",
    "    return MSE\n",
    "\n",
    "def CNN_generator(best_params, train_x, train_y):\n",
    "    \n",
    "    #Extract the best parameters\n",
    "    activation_fx = best_params['activation']\n",
    "    conv_filters = best_params['filters']\n",
    "    LSTM_layers = best_params['LSTM_layers']\n",
    "    dense_layers = best_params['Dense_layers']\n",
    "    lr = best_params['learning_rate']\n",
    "    reg = best_params['reg']\n",
    "    droprate = best_params['dropout']\n",
    "    epochs = best_params['epochs']\n",
    "    batch_n = best_params['batch_size']\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    #Setup the model structure\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(conv_filters, conv_filters, activation = activation_fx,\n",
    "                     padding = 'valid', input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dense_layers, activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(dense_layers/2), activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_x.shape[2]*train_x.shape[2], activation='linear'))\n",
    "    model.add(Reshape((train_x.shape[2], train_x.shape[2])))\n",
    "    \n",
    "    model.compile(optimizer = Adam(learning_rate = lr), loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    history = model.fit(train_x, train_y, validation_split = 0.2, shuffle = True, \n",
    "              batch_size = batch_n, epochs = epochs, verbose = 0, callbacks=[es])\n",
    "   \n",
    "    return model, history \n",
    "\n",
    "def CNN_evaluator(best_params, train_x, train_y, predict_x):\n",
    "    model, history = CNN_generator(best_params, train_x, train_y)\n",
    "    iter_pred = model.predict(predict_x, verbose = 0)\n",
    "    return iter_pred, model, history\n",
    "\n",
    "\"\"\"\n",
    "def objective_RNN(trial, RNN_train_x=RNN_train_x, RNN_train_y=RNN_train_y):\n",
    "    K.clear_session() \n",
    "            \n",
    "    #Setup the model hyperparameter search\n",
    "    activation_fx = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"LeakyReLU\"])\n",
    "    LSTM_layers = trial.suggest_categorical(\"LSTM_layers\", [i for i in range(10, 100, 10)])\n",
    "    dense_layers = trial.suggest_categorical(\"Dense_layers\", [train_x.shape[2]*i for i in range(10)])\n",
    "    lr = trial.suggest_categorical('learning_rate', [.001, .01, .1])\n",
    "    reg = trial.suggest_categorical('reg', [10**-3, 10**-5])\n",
    "    droprate = trial.suggest_categorical('dropout', [.20, .30, .4])\n",
    "    epochs = trial.suggest_categorical('epochs', [100])\n",
    "    batch_n = trial.suggest_categorical('batch_size', [64, 64*2])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "    \n",
    "    #Setup the model structure\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dense_layers, activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(dense_layers/2), activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_x.shape[2]*train_x.shape[2], activation='linear'))\n",
    "    model.add(Reshape((train_x.shape[2], train_x.shape[2])))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=\"mean_squared_error\", metrics='mean_squared_error')\n",
    "    model.fit(RNN_train_x, RNN_train_y, validation_split = 0.2, shuffle = True, batch_size = batch_n, \n",
    "              epochs = epochs, verbose = 0, callbacks=[es]) \n",
    "    MSE = ts_RNN_cross_validation(RNN_train_x, RNN_train_y, model)\n",
    "    return MSE\n",
    "\n",
    "def RNN_generator(best_params, train_x, train_y):\n",
    "    \n",
    "    #Extract the best parameters\n",
    "    activation_fx = best_params['activation']\n",
    "    conv_filters = best_params['filters']\n",
    "    LSTM_layers = best_params['LSTM_layers']\n",
    "    dense_layers = best_params['Dense_layers']\n",
    "    lr = best_params['learning_rate']\n",
    "    reg = best_params['reg']\n",
    "    droprate = best_params['dropout']\n",
    "    epochs = best_params['epochs']\n",
    "    batch_n = best_params['batch_size']\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience = 10, restore_best_weights=True)\n",
    "    \n",
    "    #Setup the model structure\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(LSTM(LSTM_layers, activation=activation_fx, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(dense_layers, activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(dense_layers/2), activation=activation_fx, kernel_regularizer= regularizers.L1L2(l1=reg, l2=reg)))\n",
    "    model.add(Dropout(droprate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(train_x.shape[2]*train_x.shape[2], activation='linear'))\n",
    "    model.add(Reshape((train_x.shape[2], train_x.shape[2])))\n",
    "    \n",
    "    model.compile(optimizer = Adam(learning_rate = lr), loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    history = model.fit(train_x, train_y, validation_split = 0.2, shuffle = True, \n",
    "              batch_size = batch_n, epochs = epochs, verbose = 0, callbacks=[es])\n",
    "   \n",
    "    return model, history \n",
    "\n",
    "def RNN_evaluator(best_params, train_x, train_y, predict_x):\n",
    "    model, history = RNN_generator(best_params, train_x, train_y)\n",
    "    iter_pred = model.predict(predict_x, verbose = 0)\n",
    "    return iter_pred, model, history\n",
    "\"\"\"\n",
    "def backtest_CNN(X_df, y_df, min_train_window, re_train_periods, cov_window, runs):\n",
    "    #Convert all observations into trailing dates\n",
    "    y_train, X_train, dates = RNN_feeder_array(X_df, y_df, cov_window)\n",
    "    prediction_df = pd.DataFrame(columns = [col for col in y_df.columns])\n",
    "       \n",
    "    for i in range(min_train_window,len(dates)):\n",
    "        train_x = X_train[0:(i-1)]\n",
    "        train_y = y_train[0:(i-1)]\n",
    "        predict_x = X_train[i]\n",
    "        predict_y = y_train[i]\n",
    "        iter_index = y_df[y_df.index.get_level_values('Date') == dates[i]].index\n",
    "    \n",
    "       \n",
    "        \"\"\"\n",
    "        if i % re_train_periods == 0:\n",
    "            #Train the CNN Model\n",
    "            start_time = time.time()\n",
    "            study = optimization_instance(objective_CNN, runs)\n",
    "            train_x, predict_x = array_robust_scaler(train_x, predict_x.reshape(1, predict_x.shape[0], predict_x.shape[1]))\n",
    "            y_pred, predict_model, history = CNN_evaluator(study.best_params, train_x, train_y, predict_x)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\n",
    "            save_predictions(prediction_df, 'CNN', str(dates[i])[0:10], working_path)\n",
    "            save_NN_model(predict_model, 'CNN', str(dates[i])[0:10], working_path)\n",
    "            end_time = time.time()\n",
    "            print(\"One training instance takes: \", (end_time - start_time)/60.0)\n",
    "            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\n",
    "        \n",
    "        else:\n",
    "            train_x, predict_x = array_robust_scaler(train_x, predict_x.reshape(1, predict_x.shape[0], predict_x.shape[1]))\n",
    "            y_pred = predict_model.predict(predict_x, verbose = 0)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "    \n",
    "    save_predictions(prediction_df, 'CNN', '_Final', working_path)\n",
    "    print(\"Run is now fully complete.\")\n",
    "    \"\"\"\n",
    "    return train_x, train_y, predict_x, predict_y\n",
    "\n",
    "\"\"\"\n",
    "def backtest_RNN(X_df, y_df, min_train_window, re_train_periods, cov_window, runs):\n",
    "    #Convert all observations into trailing dates\n",
    "    y_train, X_train, dates = RNN_feeder_array(X_df, y_df, cov_window)\n",
    "    prediction_df = pd.DataFrame(columns = [col for col in y_df.columns])\n",
    "    \n",
    "    for i in range(min_train_window,len(dates)):\n",
    "        RNN_train_x = X_train[0:(i-1)]\n",
    "        RNN_train_y = y_train[0:(i-1)]\n",
    "        RNN_predict_x = X_train[i]\n",
    "        RNN_predict_y = y_train[i]\n",
    "        iter_index = y_df[y_df.index.get_level_values('Date') == dates[i]].index\n",
    "        scaled_train, scaled_predict = array_robust_scaler(RNN_train_x, RNN_predict_x.reshape(1, RNN_predict_x.shape[0], RNN_predict_x.shape[1]))\n",
    "\n",
    "        if i % re_train_periods == 0:\n",
    "            #Train the RNN Model\n",
    "            start_time = time.time()\n",
    "            study = optimization_instance(objective_RNN, runs)\n",
    "            y_pred, predict_model, history = RNN_evaluator(study.best_params, scaled_train, RNN_train_y, scaled_predict)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\n",
    "            save_predictions(prediction_df, 'RNN', str(dates[i])[0:10], working_path)\n",
    "            save_NN_model(predict_model, 'RNN', str(dates[i])[0:10], working_path)\n",
    "            end_time = time.time()\n",
    "            print(\"One training instance takes: \", (end_time - start_time)/60.0)\n",
    "            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\n",
    "        \n",
    "        else:\n",
    "            y_pred = predict_model.predict(scaled_predict, verbose = 0)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "    \n",
    "    save_predictions(prediction_df, 'RNN', '_Final', working_path)\n",
    "    print(\"Run is now fully complete.\")\n",
    "    return prediction_df\n",
    "\n",
    "\"\"\"\n",
    "#CNN_prediction_df = backtest_CNN(combined_df[test_list], 252, 252, 1)\n",
    "#CNN_prediction_df  = backtest_CNN(X_df, y_df, initial_train_window, retrain_intervals, cov_window, 1)\n",
    "\n",
    "#train_x, train_y, predict_x, predict_y = backtest_CNN(X_df, y_df, initial_train_window, retrain_intervals, cov_window, 1)\n",
    "#RNN_prediction_df = backtest_RNN(X_df, y_df, initial_train_window, retrain_intervals, cov_window, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_CNN(X_df, y_df, min_train_window, re_train_periods, cov_window, runs):\n",
    "    #Convert all observations into trailing dates\n",
    "    y_train, X_train, dates = RNN_feeder_array(X_df, y_df, cov_window)\n",
    "    prediction_df = pd.DataFrame(columns = [col for col in y_df.columns])\n",
    "    \n",
    "    \n",
    "    for i in range(min_train_window,len(dates)):\n",
    "        train_x = X_train[0:(i-cov_window)] #Avoid overlaps with any of the predict window\n",
    "        train_y = y_train[0:(i-cov_window)] #Avoid overalps with any of the predict window\n",
    "        predict_x = X_train[i]\n",
    "        predict_x = predict_x.reshape(1, predict_x.shape[0], predict_x.shape[1])\n",
    "        predict_y = y_train[i]\n",
    "        iter_index = y_df[y_df.index.get_level_values('Date') == dates[i]].index\n",
    "        \n",
    "        if i % re_train_periods == 0:\n",
    "            #Train the CNN Model\n",
    "            start_time = time.time()\n",
    "            study = optimization_instance(objective_CNN, runs)\n",
    "            #scaled_train, scaled_predict = array_robust_scaler(train_x, predict_x.reshape(1, predict_x.shape[0], predict_x.shape[1]))\n",
    "            y_pred, predict_model, history = CNN_evaluator(study.best_params, train_x, train_y, predict_x)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "            working_path = \"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/\"\n",
    "            save_predictions(prediction_df, 'CNN', str(dates[i])[0:10], working_path)\n",
    "            save_NN_model(predict_model, 'CNN', str(dates[i])[0:10], working_path)\n",
    "            end_time = time.time()\n",
    "            print(\"One training instance takes: \", (end_time - start_time)/60.0)\n",
    "            print(str(np.round(i/len(dates) ,2)), \" of the run is complete.\")\n",
    "        \n",
    "        else:\n",
    "            #scaled_train, scaled_predict = array_robust_scaler(train_x, predict_x.reshape(1, predict_x.shape[0], predict_x.shape[1]))\n",
    "            y_pred = predict_model.predict(predict_x, verbose = 0)\n",
    "            y_pred = y_pred.reshape(y_df.shape[1], y_df.shape[1])\n",
    "            prediction_df = pd.concat([prediction_df, pd.DataFrame(y_pred, columns = y_df.columns, index = iter_index)])\n",
    "        \n",
    "    save_predictions(prediction_df, 'CNN', '_Final', working_path)\n",
    "    print(\"Run is now fully complete.\")\n",
    "    \n",
    "    return y_pred, predict_model, history, train_x, predict_x, study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training instance takes:  889.7617967327436\n",
      "0.08  of the run is complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training instance takes:  1799.7092478871346\n",
      "0.12  of the run is complete.\n"
     ]
    }
   ],
   "source": [
    "y_pred, predict_model, history, train_x, predict_x, study = backtest_CNN(X_df, y_df, initial_train_window, retrain_intervals, cov_window, CV_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/andrew_lazzeri/Desktop/MultiAsset Covariance Matrix/Prediction_dfs/CNN__Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Date', 'Commodity']] = test['Unnamed: 0'].str.split(',', expand =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.set_index(['Date', 'Commodity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iter_cols = list(set(test.index.get_level_values('Commodity')))\n",
    "\n",
    "for col in iter_cols:\n",
    "    iter_df = test[test.index.get_level_values('Commodity') == col]\n",
    "    for col2 in iter_df.columns:\n",
    "        plt.plot(iter_df[col2].values)\n",
    "        plt.title(col2)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
